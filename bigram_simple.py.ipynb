{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab35220",
   "metadata": {},
   "source": [
    "##  Load the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9efc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "words = open('data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f51796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aeea0-c5ac-4eab-9161-91413bc956f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bigram count model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d1250",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Build the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d06de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beff1fd",
   "metadata": {},
   "source": [
    "### Build empty bigram counter and the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf69c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bigram_count[i,j] = num of occurrence where character i is followed by character j\n",
    "bigram_count = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "# This marks beginning or end\n",
    "SPECIAL_CHAR = '.'\n",
    "CHAR_TO_ID_STR = SPECIAL_CHAR + 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "char_to_id = {}\n",
    "id_to_char = []\n",
    "\n",
    "for i in range(len(CHAR_TO_ID_STR)):\n",
    "    c = CHAR_TO_ID_STR[i]\n",
    "    id_to_char.append(c)\n",
    "    char_to_id[c] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70888265",
   "metadata": {},
   "source": [
    "### Fill in the bigram counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de2e8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    bigram_count[char_to_id[SPECIAL_CHAR], char_to_id[word[0]]] += 1\n",
    "    for left in range(len(word) - 1):\n",
    "        bigram_count[char_to_id[word[left]], char_to_id[word[left + 1]]] += 1\n",
    "    bigram_count[char_to_id[word[-1]], char_to_id[SPECIAL_CHAR]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7fed61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48996f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sample from the bigram counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d475fd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "janasah\n",
      "p\n",
      "cony\n",
      "a\n",
      "nn\n",
      "kohin\n",
      "tolian\n",
      "juee\n",
      "ksahnaauranilevias\n",
      "dedainrwieta\n",
      "ssonielylarte\n",
      "faveumerifontume\n",
      "phynslenaruani\n",
      "core\n",
      "yaenon\n",
      "ka\n",
      "jabdinerimikimaynin\n",
      "anaasn\n",
      "ssorionsush\n"
     ]
    }
   ],
   "source": [
    "def get_sample(bigram_count):\n",
    "    result = []\n",
    "    cur_char_id = char_to_id[SPECIAL_CHAR]\n",
    "    while True:\n",
    "        proba_dist = bigram_count[cur_char_id].float()\n",
    "        # You don't have to normalize\n",
    "        # proba_dist /= proba_dist.sum()\n",
    "        cur_char_id = torch.multinomial(proba_dist, num_samples = 1, replacement=True, generator=g).item()\n",
    "        if id_to_char[cur_char_id] == SPECIAL_CHAR:\n",
    "            break\n",
    "        result.append(id_to_char[cur_char_id])\n",
    "    return ''.join(result)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "num_samples = 20\n",
    "for sample_i in range(num_samples):\n",
    "    print(get_sample(bigram_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e24289-5957-4b78-a2e5-6f0bb40abf35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "957d2a87-be3e-4e1c-9de2-d316176dac93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize bigram count so P[i,j] = P(char[i] is next | char[j] is previous) = count [i then j] / count[i]\n",
    "P = bigram_count / bigram_count.sum(dim = 1, keepdim=True)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f6de58f-170f-4ee6-90d0-fe527a8046b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32033],\n",
       "        [33885],\n",
       "        [ 2645],\n",
       "        [ 3532],\n",
       "        [ 5496],\n",
       "        [20423],\n",
       "        [  905],\n",
       "        [ 1927],\n",
       "        [ 7616],\n",
       "        [17701],\n",
       "        [ 2900],\n",
       "        [ 5040],\n",
       "        [13958],\n",
       "        [ 6642],\n",
       "        [18327],\n",
       "        [ 7934],\n",
       "        [ 1026],\n",
       "        [  272],\n",
       "        [12700],\n",
       "        [ 8106],\n",
       "        [ 5570],\n",
       "        [ 3135],\n",
       "        [ 2573],\n",
       "        [  929],\n",
       "        [  697],\n",
       "        [ 9776],\n",
       "        [ 2398]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count.sum(dim = 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d015f785-cbc4-484b-a1d1-e5135f48eacd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b38513db-515a-405f-9ab7-2d8ed373b636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-559891.7500)\n",
      "tensor(2.4541)\n"
     ]
    }
   ],
   "source": [
    "def get_proba(P, word, index):\n",
    "    return \n",
    "# loss = sum of neg LL\n",
    "log_l = 0\n",
    "num_transitions = 0\n",
    "for word in words:\n",
    "    word = SPECIAL_CHAR + word + SPECIAL_CHAR\n",
    "    for left in range(len(word)-1):\n",
    "        log_l += torch.log(P[char_to_id[word[left]], char_to_id[word[left + 1]]])\n",
    "        num_transitions += 1\n",
    "        \n",
    "# loss = average negative LL\n",
    "\n",
    "loss = -log_l / num_transitions\n",
    "print(log_l)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8cdcd-bedf-464e-a0b8-ad4d527484ed",
   "metadata": {},
   "source": [
    "# 1-linear logit NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664f1e0-a5f8-4547-b28b-2657eb6e315e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Just focus on the first word, which has 5 transitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "89ad88d7-4da7-49ea-9e52-702915369b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e816e799-2741-4561-8778-103589ffcab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "training_word = SPECIAL_CHAR + words[0] + SPECIAL_CHAR\n",
    "xs = []\n",
    "ys = []\n",
    "for left in range(len(training_word)-1):\n",
    "    xs.append(char_to_id[training_word[left]])\n",
    "    ys.append(char_to_id[training_word[left+1]])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f93b5-8042-46e7-823c-0d08deed39b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model and loss (step by step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9a149cfe-7293-458b-a56b-91fdae775da6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 0. Input = 5 integers, which corresponds to an input batch of size 5, and each item is the previous char's ID.\n",
    "# 1. One-hot encode\n",
    "import torch.nn.functional as F\n",
    "# The float() is needed, or else xs @ W will fail: incompatible types\n",
    "xs_encoded = F.one_hot(xs, num_classes = len(char_to_id)).float()\n",
    "print(xs_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3d999bc1-95ba-46dd-a25b-e8fa33a77651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. linear layer.\n",
    "W = torch.randn((27, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a531b148-cfd6-4f81-a63d-fd3828967b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4334,  0.7718, -0.7763, -0.5231,  3.8199,  2.1374, -1.0169,  0.3291,\n",
      "         -0.2065,  0.3236, -0.7128,  0.4365,  0.7066, -0.7783, -2.0312,  1.1396,\n",
      "         -0.6401, -1.2754, -0.6002,  0.8198, -0.4853,  0.1190, -0.2811,  1.3456,\n",
      "          0.7820,  1.3204,  0.3893],\n",
      "        [-1.6037, -1.2664, -0.6748,  0.2757, -0.1345, -0.0398,  1.3545, -1.3284,\n",
      "         -0.4246,  1.0885, -0.7935,  0.3824, -0.2161,  1.5545, -0.7669, -0.6485,\n",
      "         -0.1323,  1.3386,  1.3518,  1.0557, -0.6167,  1.2291,  0.7124,  0.2874,\n",
      "         -0.6726, -0.2079, -0.0234],\n",
      "        [-2.3104,  0.0854, -0.1798, -0.5887, -0.3187, -0.0692,  1.2934, -0.1856,\n",
      "         -1.2556,  2.6381, -1.0469, -1.0034,  0.8970, -0.5868,  0.2359, -0.0210,\n",
      "         -1.0383, -0.8930,  0.0863, -0.1942, -0.7802,  1.3803, -1.1363,  0.8544,\n",
      "          1.8015,  0.2029,  0.9579],\n",
      "        [-2.3104,  0.0854, -0.1798, -0.5887, -0.3187, -0.0692,  1.2934, -0.1856,\n",
      "         -1.2556,  2.6381, -1.0469, -1.0034,  0.8970, -0.5868,  0.2359, -0.0210,\n",
      "         -1.0383, -0.8930,  0.0863, -0.1942, -0.7802,  1.3803, -1.1363,  0.8544,\n",
      "          1.8015,  0.2029,  0.9579],\n",
      "        [ 0.8458,  1.0007,  0.1810, -1.2760,  0.1720,  1.0507,  1.2129, -0.8217,\n",
      "          1.1364, -0.9975, -0.5931,  1.0355,  2.3742,  1.1836, -1.2851, -1.4264,\n",
      "         -1.2222,  0.1536, -0.1524,  0.7303, -0.0288,  1.9037, -0.1828, -0.5342,\n",
      "         -1.3056, -0.7012, -0.1586]])\n"
     ]
    }
   ],
   "source": [
    "logits = xs_encoded @ W\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "66dc9548-f83b-4e8a-9bcf-e3ff3717767b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1166, 0.0221, 0.0047, 0.0061, 0.4663, 0.0867, 0.0037, 0.0142, 0.0083,\n",
      "         0.0141, 0.0050, 0.0158, 0.0207, 0.0047, 0.0013, 0.0320, 0.0054, 0.0029,\n",
      "         0.0056, 0.0232, 0.0063, 0.0115, 0.0077, 0.0393, 0.0224, 0.0383, 0.0151],\n",
      "        [0.0049, 0.0068, 0.0123, 0.0318, 0.0211, 0.0232, 0.0936, 0.0064, 0.0158,\n",
      "         0.0717, 0.0109, 0.0354, 0.0195, 0.1143, 0.0112, 0.0126, 0.0212, 0.0921,\n",
      "         0.0933, 0.0694, 0.0130, 0.0825, 0.0492, 0.0322, 0.0123, 0.0196, 0.0236],\n",
      "        [0.0020, 0.0224, 0.0172, 0.0114, 0.0150, 0.0192, 0.0750, 0.0171, 0.0059,\n",
      "         0.2876, 0.0072, 0.0075, 0.0504, 0.0114, 0.0260, 0.0201, 0.0073, 0.0084,\n",
      "         0.0224, 0.0169, 0.0094, 0.0818, 0.0066, 0.0483, 0.1246, 0.0252, 0.0536],\n",
      "        [0.0020, 0.0224, 0.0172, 0.0114, 0.0150, 0.0192, 0.0750, 0.0171, 0.0059,\n",
      "         0.2876, 0.0072, 0.0075, 0.0504, 0.0114, 0.0260, 0.0201, 0.0073, 0.0084,\n",
      "         0.0224, 0.0169, 0.0094, 0.0818, 0.0066, 0.0483, 0.1246, 0.0252, 0.0536],\n",
      "        [0.0458, 0.0535, 0.0236, 0.0055, 0.0233, 0.0562, 0.0661, 0.0086, 0.0612,\n",
      "         0.0072, 0.0109, 0.0554, 0.2112, 0.0642, 0.0054, 0.0047, 0.0058, 0.0229,\n",
      "         0.0169, 0.0408, 0.0191, 0.1319, 0.0164, 0.0115, 0.0053, 0.0097, 0.0168]])\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# 3. Softmax the logits to get probabilities. This is where you will do sampling from!\n",
    "probas = F.softmax(logits, dim=1)\n",
    "print(probas)\n",
    "# Note dim = 1 means summing where dim 1 = : will be 1. \n",
    "print(probas[2,:].sum())\n",
    "print(probas[1,:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2609c5f7-acf0-4b03-8e7b-9ec0bc1f9058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model above is already fully defined, but for training, we need to define more layers to get the loss fcn, so we can do bprop\n",
    "# Get the probas corresponding to the gold label ys\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1f2a9253-7e4f-4e13-b436-24eeef455c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2252e535-a2a1-4144-afbf-7aecaeabb5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0263, 0.0134, 0.0364, 0.0558, 0.0076])\n"
     ]
    }
   ],
   "source": [
    "selected_probas = probas[torch.arange(5), ys]\n",
    "print(selected_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9e2bf4f7-635a-4eee-87ab-ebb7a1a7fe3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8055)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now just do avg negative LL\n",
    "-selected_probas.log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb379bf5-75e8-48a1-a226-ff9779b01728",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77c72c10-45d7-4c32-ab7a-b8dfcefe819d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0 = 3.6588\n",
      "Loss at 50 = 2.6739\n",
      "Loss at 100 = 1.8711\n",
      "Loss at 150 = 1.3066\n",
      "Loss at 200 = 0.9578\n",
      "Loss at 250 = 0.7530\n",
      "Loss at 300 = 0.6300\n",
      "Loss at 350 = 0.5522\n",
      "Loss at 400 = 0.5000\n",
      "Loss at 450 = 0.4633\n",
      "Loss at 500 = 0.4363\n",
      "Loss at 550 = 0.4157\n",
      "Loss at 600 = 0.3996\n",
      "Loss at 650 = 0.3867\n",
      "Loss at 700 = 0.3762\n",
      "Loss at 750 = 0.3674\n",
      "Loss at 800 = 0.3600\n",
      "Loss at 850 = 0.3537\n",
      "Loss at 900 = 0.3483\n",
      "Loss at 950 = 0.3436\n",
      "Loss at 1000 = 0.3394\n",
      "Loss at 1050 = 0.3357\n",
      "Loss at 1100 = 0.3324\n",
      "Loss at 1150 = 0.3295\n",
      "Loss at 1200 = 0.3269\n",
      "Loss at 1250 = 0.3245\n",
      "Loss at 1300 = 0.3223\n",
      "Loss at 1350 = 0.3203\n",
      "Loss at 1400 = 0.3185\n",
      "Loss at 1450 = 0.3168\n",
      "Loss at 1500 = 0.3152\n",
      "Loss at 1550 = 0.3138\n",
      "Loss at 1600 = 0.3125\n",
      "Loss at 1650 = 0.3112\n",
      "Loss at 1700 = 0.3101\n",
      "Loss at 1750 = 0.3090\n",
      "Loss at 1800 = 0.3080\n",
      "Loss at 1850 = 0.3070\n",
      "Loss at 1900 = 0.3061\n",
      "Loss at 1950 = 0.3053\n",
      "Loss at 2000 = 0.3045\n",
      "Loss at 2050 = 0.3037\n",
      "Loss at 2100 = 0.3030\n",
      "Loss at 2150 = 0.3023\n",
      "Loss at 2200 = 0.3017\n",
      "Loss at 2250 = 0.3011\n",
      "Loss at 2300 = 0.3005\n",
      "Loss at 2350 = 0.3000\n",
      "Loss at 2400 = 0.2994\n",
      "Loss at 2450 = 0.2989\n",
      "Loss at 2500 = 0.2984\n",
      "Loss at 2550 = 0.2980\n",
      "Loss at 2600 = 0.2975\n",
      "Loss at 2650 = 0.2971\n",
      "Loss at 2700 = 0.2967\n",
      "Loss at 2750 = 0.2963\n",
      "Loss at 2800 = 0.2959\n",
      "Loss at 2850 = 0.2956\n",
      "Loss at 2900 = 0.2952\n",
      "Loss at 2950 = 0.2949\n",
      "Loss at 3000 = 0.2946\n",
      "Loss at 3050 = 0.2943\n",
      "Loss at 3100 = 0.2940\n",
      "Loss at 3150 = 0.2937\n",
      "Loss at 3200 = 0.2934\n",
      "Loss at 3250 = 0.2931\n",
      "Loss at 3300 = 0.2929\n",
      "Loss at 3350 = 0.2926\n",
      "Loss at 3400 = 0.2924\n",
      "Loss at 3450 = 0.2921\n",
      "Loss at 3500 = 0.2919\n",
      "Loss at 3550 = 0.2917\n",
      "Loss at 3600 = 0.2915\n",
      "Loss at 3650 = 0.2913\n",
      "Loss at 3700 = 0.2911\n",
      "Loss at 3750 = 0.2909\n",
      "Loss at 3800 = 0.2907\n",
      "Loss at 3850 = 0.2905\n",
      "Loss at 3900 = 0.2903\n",
      "Loss at 3950 = 0.2901\n",
      "Loss at 4000 = 0.2899\n",
      "Loss at 4050 = 0.2898\n",
      "Loss at 4100 = 0.2896\n",
      "Loss at 4150 = 0.2895\n",
      "Loss at 4200 = 0.2893\n",
      "Loss at 4250 = 0.2891\n",
      "Loss at 4300 = 0.2890\n",
      "Loss at 4350 = 0.2889\n",
      "Loss at 4400 = 0.2887\n",
      "Loss at 4450 = 0.2886\n",
      "Loss at 4500 = 0.2884\n",
      "Loss at 4550 = 0.2883\n",
      "Loss at 4600 = 0.2882\n",
      "Loss at 4650 = 0.2881\n",
      "Loss at 4700 = 0.2879\n",
      "Loss at 4750 = 0.2878\n",
      "Loss at 4800 = 0.2877\n",
      "Loss at 4850 = 0.2876\n",
      "Loss at 4900 = 0.2875\n",
      "Loss at 4950 = 0.2874\n",
      "Loss at 5000 = 0.2873\n",
      "Loss at 5050 = 0.2872\n",
      "Loss at 5100 = 0.2870\n",
      "Loss at 5150 = 0.2869\n",
      "Loss at 5200 = 0.2869\n",
      "Loss at 5250 = 0.2868\n",
      "Loss at 5300 = 0.2867\n",
      "Loss at 5350 = 0.2866\n",
      "Loss at 5400 = 0.2865\n",
      "Loss at 5450 = 0.2864\n",
      "Loss at 5500 = 0.2863\n",
      "Loss at 5550 = 0.2862\n",
      "Loss at 5600 = 0.2861\n",
      "Loss at 5650 = 0.2860\n",
      "Loss at 5700 = 0.2860\n",
      "Loss at 5750 = 0.2859\n",
      "Loss at 5800 = 0.2858\n",
      "Loss at 5850 = 0.2857\n",
      "Loss at 5900 = 0.2857\n",
      "Loss at 5950 = 0.2856\n",
      "Loss at 6000 = 0.2855\n",
      "Loss at 6050 = 0.2854\n",
      "Loss at 6100 = 0.2854\n",
      "Loss at 6150 = 0.2853\n",
      "Loss at 6200 = 0.2852\n",
      "Loss at 6250 = 0.2852\n",
      "Loss at 6300 = 0.2851\n",
      "Loss at 6350 = 0.2850\n",
      "Loss at 6400 = 0.2850\n",
      "Loss at 6450 = 0.2849\n",
      "Loss at 6500 = 0.2848\n",
      "Loss at 6550 = 0.2848\n",
      "Loss at 6600 = 0.2847\n",
      "Loss at 6650 = 0.2847\n",
      "Loss at 6700 = 0.2846\n",
      "Loss at 6750 = 0.2845\n",
      "Loss at 6800 = 0.2845\n",
      "Loss at 6850 = 0.2844\n",
      "Loss at 6900 = 0.2844\n",
      "Loss at 6950 = 0.2843\n",
      "Loss at 7000 = 0.2843\n",
      "Loss at 7050 = 0.2842\n",
      "Loss at 7100 = 0.2842\n",
      "Loss at 7150 = 0.2841\n",
      "Loss at 7200 = 0.2841\n",
      "Loss at 7250 = 0.2840\n",
      "Loss at 7300 = 0.2840\n",
      "Loss at 7350 = 0.2839\n",
      "Loss at 7400 = 0.2839\n",
      "Loss at 7450 = 0.2838\n",
      "Loss at 7500 = 0.2838\n",
      "Loss at 7550 = 0.2837\n",
      "Loss at 7600 = 0.2837\n",
      "Loss at 7650 = 0.2837\n",
      "Loss at 7700 = 0.2836\n",
      "Loss at 7750 = 0.2836\n",
      "Loss at 7800 = 0.2835\n",
      "Loss at 7850 = 0.2835\n",
      "Loss at 7900 = 0.2834\n",
      "Loss at 7950 = 0.2834\n",
      "Loss at 8000 = 0.2834\n",
      "Loss at 8050 = 0.2833\n",
      "Loss at 8100 = 0.2833\n",
      "Loss at 8150 = 0.2833\n",
      "Loss at 8200 = 0.2832\n",
      "Loss at 8250 = 0.2832\n",
      "Loss at 8300 = 0.2831\n",
      "Loss at 8350 = 0.2831\n",
      "Loss at 8400 = 0.2831\n",
      "Loss at 8450 = 0.2830\n",
      "Loss at 8500 = 0.2830\n",
      "Loss at 8550 = 0.2830\n",
      "Loss at 8600 = 0.2829\n",
      "Loss at 8650 = 0.2829\n",
      "Loss at 8700 = 0.2829\n",
      "Loss at 8750 = 0.2828\n",
      "Loss at 8800 = 0.2828\n",
      "Loss at 8850 = 0.2828\n",
      "Loss at 8900 = 0.2827\n",
      "Loss at 8950 = 0.2827\n",
      "Loss at 9000 = 0.2827\n",
      "Loss at 9050 = 0.2826\n",
      "Loss at 9100 = 0.2826\n",
      "Loss at 9150 = 0.2826\n",
      "Loss at 9200 = 0.2825\n",
      "Loss at 9250 = 0.2825\n",
      "Loss at 9300 = 0.2825\n",
      "Loss at 9350 = 0.2825\n",
      "Loss at 9400 = 0.2824\n",
      "Loss at 9450 = 0.2824\n",
      "Loss at 9500 = 0.2824\n",
      "Loss at 9550 = 0.2823\n",
      "Loss at 9600 = 0.2823\n",
      "Loss at 9650 = 0.2823\n",
      "Loss at 9700 = 0.2823\n",
      "Loss at 9750 = 0.2822\n",
      "Loss at 9800 = 0.2822\n",
      "Loss at 9850 = 0.2822\n",
      "Loss at 9900 = 0.2822\n",
      "Loss at 9950 = 0.2821\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "W = torch.randn(27,27, requires_grad = True)\n",
    "for rep in range(10000):\n",
    "    # forward pass\n",
    "    xs_enc = F.one_hot(xs, num_classes = 27).float()\n",
    "    logits = xs_enc @ W\n",
    "    probas = F.softmax(logits, dim = 1)\n",
    "    selected_probas = probas[torch.arange(len(xs)), ys]\n",
    "    loss = -selected_probas.log().mean()\n",
    "    \n",
    "    # bwd\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data -= 0.1 * W.grad\n",
    "    \n",
    "    if rep % 50 == 0:\n",
    "        print(f\"Loss at {rep} = {loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e9431-ee7e-4286-818f-3b49205fb1eb",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "693d4b9b-6ee1-47e4-b7c9-8771c0437571",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "emmma\n",
      "emmma\n",
      "emma\n",
      "ema\n",
      "ema\n",
      "emmmma\n",
      "emmmma\n",
      "ema\n",
      "ema\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "for sample_i in range(10):\n",
    "    cur_char_id = char_to_id[SPECIAL_CHAR]\n",
    "    word = []\n",
    "    while True:\n",
    "        # forward pass, but without the loss\n",
    "        input_raw = torch.tensor([cur_char_id])\n",
    "        input_enc = F.one_hot(input_raw, num_classes = 27).float()\n",
    "        logits = input_enc @ W\n",
    "        probas = F.softmax(logits, dim = 1)\n",
    "        \n",
    "        # Sample\n",
    "        assert probas.shape == (1,27), probas.shape\n",
    "        cur_char_id = torch.multinomial(probas[0], num_samples = 1, replacement=True).item()\n",
    "        if cur_char_id == char_to_id[SPECIAL_CHAR]:\n",
    "            break\n",
    "        word.append(id_to_char[cur_char_id])\n",
    "        if len(word) >= max_len:\n",
    "            break;\n",
    "    print(''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6f446-f6b1-4423-93ec-2e6bf3733e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
